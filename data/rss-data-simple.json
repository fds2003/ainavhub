{
  "articles": [
    {
      "title": "BotzoneBench: Scalable LLM Evaluation via Graded AI Anchors",
      "title_zh": "BotzoneBench: Scalable LLM Evaluation via Graded AI Anchors",
      "description": "arXiv:2602.13214v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are increasingly deployed in interactive environments requiring strategic decision-making, yet systematic evaluation of these capabilities remains challenging. Existing benchmarks for LLMs primarily assess static reasoning through isolated tasks and fail to capture dynamic strategic abilities. Recent game-based evaluations employ LLM-vs-LLM tournaments that produce relative rankings dependent on transient model pools, ",
      "link": "https://arxiv.org/abs/2602.13214",
      "source": "ArXiv AI 研究",
      "category": "学术研究",
      "date": "2026-02-17"
    },
    {
      "title": "Advance Trustworthy AI and ML, and Identify Best Practices for Scaling AI",
      "title_zh": "Advance Trustworthy AI and ML, and Identify Best Practices for Scaling AI",
      "description": "By John P. Desmond, AI Trends Editor   Advancing trustworthy AI and machine learning to mitigate agency risk is a priority for the US Department of Energy (DOE), and identifying best practices for implementing AI at scale is a priority for the US General Services Administration (GSA).   That’s what attendees learned in two sessions at the AI [&#8230;]]]>",
      "link": "https://www.aitrends.com/ai-world-government/advance-trustworthy-ai-and-ml-and-identify-best-practices-for-scaling-ai/",
      "source": "AI 趋势",
      "category": "行业趋势",
      "date": "2021-10-28"
    },
    {
      "title": "Best Practices for Building the AI Development Platform in Government",
      "title_zh": "Best Practices for Building the AI Development Platform in Government",
      "description": "By John P. Desmond, AI Trends Editor  The AI stack defined by Carnegie Mellon University is fundamental to the approach being taken by the US Army for its AI development platform efforts, according to Isaac Faber, Chief Data Scientist at the US Army AI Integration Center, speaking at the AI World Government event held in-person and virtually [&#8230;]]]>",
      "link": "https://www.aitrends.com/ai-world-government/best-practices-for-building-the-ai-development-platform-in-government/",
      "source": "AI 趋势",
      "category": "行业趋势",
      "date": "2021-10-28"
    },
    {
      "title": "How Accountability Practices Are Pursued by AI Engineers in the Federal Government",
      "title_zh": "How Accountability Practices Are Pursued by AI Engineers in the Federal Government",
      "description": "By John P. Desmond, AI Trends Editor    Two experiences of how AI developers within the federal government are pursuing AI accountability practices were outlined at the AI World Government event held virtually and in-person this week in Alexandria, Va.  Taka Ariga, chief data scientist and director at the US Government Accountability Office, described an AI accountability framework he uses within his agency [&#8230;]]]>",
      "link": "https://www.aitrends.com/ai-world-government/how-accountability-practices-are-pursued-by-ai-engineers-in-the-federal-government/",
      "source": "AI 趋势",
      "category": "行业趋势",
      "date": "2021-10-21"
    },
    {
      "title": "Agentic AI for Commercial Insurance Underwriting with Adversarial Self-Critique",
      "title_zh": "Agentic AI for Commercial Insurance Underwriting with Adversarial Self-Critique",
      "description": "arXiv:2602.13213v1 Announce Type: new \nAbstract: Commercial insurance underwriting is a labor-intensive process that requires manual review of extensive documentation to assess risk and determine policy pricing. While AI offers substantial efficiency improvements, existing solutions lack comprehensive reasoning capabilities and internal mechanisms to ensure reliability within regulated, high-stakes environments. Full automation remains impractical and inadvisable in scenarios where human judgmen",
      "link": "https://arxiv.org/abs/2602.13213",
      "source": "ArXiv AI 研究",
      "category": "学术研究",
      "date": "2026-02-17"
    },
    {
      "title": "Getting Government AI Engineers to Tune into AI Ethics Seen as Challenge",
      "title_zh": "Getting Government AI Engineers to Tune into AI Ethics Seen as Challenge",
      "description": "By John P. Desmond, AI Trends Editor   Engineers tend to see things in unambiguous terms, which some may call Black and White terms, such as a choice between right or wrong and good and bad. The consideration of ethics in AI is highly nuanced, with vast gray areas, making it  challenging for AI software engineers to [&#8230;]]]>",
      "link": "https://www.aitrends.com/ai-world-government/getting-government-ai-engineers-to-tune-into-ai-ethics-seen-as-challenge/",
      "source": "AI 趋势",
      "category": "行业趋势",
      "date": "2021-10-21"
    },
    {
      "title": "Novelty In The Game Of Go Provides Bright Insights For AI And Autonomous Vehicles",
      "title_zh": "Novelty In The Game Of Go Provides Bright Insights For AI And Autonomous Vehicles",
      "description": "By Lance Eliot, the AI Trends Insider   We already expect that humans to exhibit flashes of brilliance. It might not happen all the time, but the act itself is welcomed and not altogether disturbing when it occurs.    What about when Artificial Intelligence (AI) seems to display an act of novelty? Any such instance is bound to get our attention; [&#8230;]]]>",
      "link": "https://www.aitrends.com/ai-insider/novelty-in-the-game-of-go-provides-bright-insights-for-ai-and-autonomous-vehicles/",
      "source": "AI 趋势",
      "category": "行业趋势",
      "date": "2021-10-28"
    },
    {
      "title": "Promise and Perils of Using AI for Hiring: Guard Against Data Bias",
      "title_zh": "Promise and Perils of Using AI for Hiring: Guard Against Data Bias",
      "description": "By AI Trends Staff   While AI in hiring is now widely used for writing job descriptions, screening candidates, and automating interviews, it poses a risk of wide discrimination if not implemented carefully.  That was the message from Keith Sonderling, Commissioner with the US Equal Opportunity Commision, speaking at the AI World Government event held live and virtually in [&#8230;]]]>",
      "link": "https://www.aitrends.com/ai-world-government/promise-and-perils-of-using-ai-for-hiring-guard-against-data-bias/",
      "source": "AI 趋势",
      "category": "行业趋势",
      "date": "2021-10-28"
    },
    {
      "title": "Predictive Maintenance Proving Out as Successful AI Use Case",
      "title_zh": "Predictive Maintenance Proving Out as Successful AI Use Case",
      "description": "By John P. Desmond, AI Trends Editor   More companies are successfully exploiting predictive maintenance systems that combine AI and IoT sensors to collect data that anticipates breakdowns and recommends preventive action before break or machines fail, in a demonstration of an AI use case with proven value.   This growth is reflected in optimistic market forecasts. [&#8230;]]]>",
      "link": "https://www.aitrends.com/predictive-analytics/predictive-maintenance-proving-out-as-successful-ai-use-case/",
      "source": "AI 趋势",
      "category": "行业趋势",
      "date": "2021-10-28"
    },
    {
      "title": "Digital Natives Seen Having Advantages as Part of Government AI Engineering Teams",
      "title_zh": "Digital Natives Seen Having Advantages as Part of Government AI Engineering Teams",
      "description": "By John P. Desmond, AI Trends Editor   AI is more accessible to young people in the workforce who grew up as ‘digital natives’ with Alexa and self-driving cars as part of the landscape, giving them expectations grounded in their experience of what is possible.   That idea set the foundation for a panel discussion at AI World [&#8230;]]]>",
      "link": "https://www.aitrends.com/ai-world-government/digital-natives-seen-having-advantages-as-part-of-government-ai-engineering-teams/",
      "source": "AI 趋势",
      "category": "行业趋势",
      "date": "2021-10-21"
    },
    {
      "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
      "title_zh": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
      "description": "arXiv:2602.13226v1 Announce Type: new \nAbstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and practical LLM-generated text detection method, VaryBalance. The core of VaryBalance is that, compared to LLM-generated texts, there is a greater differe",
      "link": "https://arxiv.org/abs/2602.13226",
      "source": "ArXiv AI 研究",
      "category": "学术研究",
      "date": "2026-02-17"
    },
    {
      "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
      "title_zh": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
      "description": "arXiv:2602.13232v1 Announce Type: new \nAbstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-form captioning. PlotChain contains 15 plot families with 450 rendered plots (30 per family), where every item is produced from known parameters and pai",
      "link": "https://arxiv.org/abs/2602.13232",
      "source": "ArXiv AI 研究",
      "category": "学术研究",
      "date": "2026-02-17"
    },
    {
      "title": "Intelligence as Trajectory-Dominant Pareto Optimization",
      "title_zh": "Intelligence as Trajectory-Dominant Pareto Optimization",
      "description": "arXiv:2602.13230v1 Announce Type: new \nAbstract: Despite recent advances in artificial intelligence, many systems exhibit stagnation in long-horizon adaptability despite continued performance optimization. This work argues that such limitations do not primarily arise from insufficient learning, data, or model capacity, but from a deeper structural property of how intelligence is optimized over time. We formulate intelligence as a trajectory-level phenomenon governed by multi-objective trade-offs",
      "link": "https://arxiv.org/abs/2602.13230",
      "source": "ArXiv AI 研究",
      "category": "学术研究",
      "date": "2026-02-17"
    },
    {
      "title": "VeRA: Verified Reasoning Data Augmentation at Scale",
      "title_zh": "VeRA: Verified Reasoning Data Augmentation at Scale",
      "description": "arXiv:2602.13217v1 Announce Type: new \nAbstract: The main issue with most evaluation schemes today is their \"static\" nature: the same problems are reused repeatedly, allowing for memorization, format exploitation, and eventual saturation. To measure genuine AI progress, we need evaluation that is robust by construction, not by post-hoc detection. In response, we propose VeRA (Verified Reasoning Data Augmentation), a framework that converts benchmark problems into executable specifications, compr",
      "link": "https://arxiv.org/abs/2602.13217",
      "source": "ArXiv AI 研究",
      "category": "学术研究",
      "date": "2026-02-17"
    },
    {
      "title": "A Geometric Taxonomy of Hallucinations in LLMs",
      "title_zh": "A Geometric Taxonomy of Hallucinations in LLMs",
      "description": "arXiv:2602.13224v1 Announce Type: new \nAbstract: The term \"hallucination\" in large language models conflates distinct phenomena with different geometric signatures in embedding space. We propose a taxonomy identifying three types: unfaithfulness (failure to engage with provided context), confabulation (invention of semantically foreign content), and factual error (incorrect claims within correct conceptual frames). We observe a striking asymmetry. On standard benchmarks where hallucinations are ",
      "link": "https://arxiv.org/abs/2602.13224",
      "source": "ArXiv AI 研究",
      "category": "学术研究",
      "date": "2026-02-17"
    },
    {
      "title": "Stay in Character, Stay Safe: Dual-Cycle Adversarial Self-Evolution for Safety Role-Playing Agents",
      "title_zh": "Stay in Character, Stay Safe: Dual-Cycle Adversarial Self-Evolution for Safety Role-Playing Agents",
      "description": "arXiv:2602.13234v1 Announce Type: new \nAbstract: LLM-based role-playing has rapidly improved in fidelity, yet stronger adherence to persona constraints commonly increases vulnerability to jailbreak attacks, especially for risky or negative personas. Most prior work mitigates this issue with training-time solutions (e.g., data curation or alignment-oriented regularization). However, these approaches are costly to maintain as personas and attack strategies evolve, can degrade in-character behavior",
      "link": "https://arxiv.org/abs/2602.13234",
      "source": "ArXiv AI 研究",
      "category": "学术研究",
      "date": "2026-02-17"
    }
  ]
}